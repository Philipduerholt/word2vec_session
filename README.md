# word2vec_session
Presentation of word embeddings @devcamp18

## Links to pre-trained word embeddings

* Download pre-trained glove word embeddings https://nlp.stanford.edu/projects/glove/
* Collection of pre-trained word embeddings

## Links to educational resources

* Word vectorization intuition. https://www.analyticsvidhya.com/blog/2017/06/word-embeddings-count-word2veec/
* The skip-gram model. http://mccormickml.com/2016/04/19/word2vec-tutorial-the-skip-gram-model/
* Why does semantic arithmetic work? http://p.migdal.pl/2017/01/06/king-man-woman-queen-why.html
* Implement a skip-gram model in Keras. http://adventuresinmachinelearning.com/word2vec-keras-tutorial/